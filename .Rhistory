bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds.csv", delim = ",")
install.packages("kknn")
# Load Libraries
library(doParallel) # Parallel Computing
library(vroom) # Loading data
library(DataExplorer) # EDA
library(patchwork) # EDA
library(inspectdf) # EDA
library(ggmosaic) # EDA
library(tidyverse) # General Use
library(tidymodels) # General Modeling
library(embed) # plogr modeling
library(lme4) # plogr modeling
library(naivebayes) # Naive Bayes modeling
library(discrim) # Naive Bayes modeling
library(kknn) # K nearest neighbors
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
knn_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
knn_prep <- prep(knn_rec)
bake(knn_prep, employee_train) %>%
slice(1:10)
# Create K Nearest Neighbors model
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_mode("classification") %>%
set_engine("kknn")
# Create KNN workflow
knn_wf <- workflow() %>%
add_recipe(knn_rec) %>%
add_model(knn_mod)
# Grid of values to tune over
knn_tg <- grid_regular(neighbors()),
# Grid of values to tune over
knn_tg <- grid_regular(neighbors(),
levels = 10)
# Split data for cross-validation (CV)
knn_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
knn_cv_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tg,
metrics = metric_set(roc_auc))
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = employee_train)
# Find best tuning parameters
knn_best_tune <- knn_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = employee_train)
# Make predictions
knn_preds <- predict(knn_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=knn_preds, file="knn_preds.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
ctree_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
ctree_prep <- prep(ctree_rec)
bake(ctree_prep, employee_train)
# MODELING ------------------------------------------------------
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(ctree_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 6)
# Split data for cross-validation (CV)
ctree_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
ctree_cv_results <- ctree_wf %>%
tune_grid(resamples = ctree_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
ctree_best_tune <- ctree_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
ctree_final_wf <- ctree_wf %>%
finalize_workflow(ctree_best_tune) %>%
fit(data = employee_train)
# PREDICTIONS ---------------------------------------------------
# Predict without a classification cutoff--just the raw probabilities
ctree_preds <- predict(ctree_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=ctree_preds, file="ctree_preds_6levels.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
ctreepcr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR w/ Threshold = .9
step_pca(all_predictors(), threshold = .9)
# Prep, Bake, and View Recipe
ctreepcr_prep <- prep(ctreepcr_rec)
bake(ctreepcr_prep, employee_train)
View(employee_train)
# Create classification forest model
ctreepcr_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctreepcr_wf <- workflow() %>%
add_recipe(ctreepcr_rec) %>%
add_model(ctreepcr_mod)
# Grid of values to tune over
ctreepcr_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 15)
# Split data for cross-validation (CV)
ctreepcr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Load Libraries
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
plogrpcr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .92)
# Prep, Bake, and View Recipe
plogrpcr_prep <- prep(plogrpcr_rec)
bake(plogrpcr_prep, employee_train) %>%
slice(1:10)
# Create Recipe
plogrpcr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
plogrpcr_prep <- prep(plogrpcr_rec)
bake(plogrpcr_prep, employee_train) %>%
slice(1:10)
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(plogrpcr_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross-validation (CV)
plogrpcr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
plogrpcr_cv_results <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
plogrpcr_best_tune <- plogrpcr_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
plogrpcr_final_wf <- plogrpcr_wf %>%
finalize_workflow(plogrpcr_best_tune) %>%
fit(data = employee_train)
# PREDICTIONS ---------------------------------------------------
# Predict without a classification cutoff--just the raw probabilities
plogrpcr_preds_no_c <- predict(plogrpcr_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=plogrpcr_preds_no_c, file="plogrpcr_preds_no_c_no_step_other.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(bike_train,
v = 5,
repeats = 1) # Split data for CV
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
library(stacks)
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(plogrpcr_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(ctree_rec) %>%
add_model(ctree_mod)
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# CLASSIFICATION FOREST MODELING ------------------------------------
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 5)
# Run cross-validation
ctree_fit <- ctree_wf %>%
tune_grid(resamples = ctree_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = stack1_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# CLASSIFICATION FOREST MODELING ------------------------------------
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 5)
# Run cross-validation
ctree_fit <- ctree_wf %>%
tune_grid(resamples = stack1_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# STACKED MODEL ----------------------------------------------
# Specify models to include
stack1_stack <- stacks() %>%
add_candidates(plogrpcr_fit) %>%
add_candidates(ctree_fit)
# Fit model w/ LASSO penalized regression meta-learner
stacked_model1 <- stack1_stack %>%
blend_predictions() %>%
fit_members()
# PREDICTIONS ---------------------------------------------------
# Predict without a classification cutoff--just the raw probabilities
stacked1_preds <- predict(stacked_model1,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=stacked1_preds, file="stacked1_preds.csv", delim = ",")
