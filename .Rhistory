step_pca(all_predictors(), threshold = .92)
# Prep, Bake, and View Recipe
plogrpcr_prep <- prep(plogrpcr_rec)
bake(plogrpcr_prep, employee_train) %>%
slice(1:10)
# Create Recipe
plogrpcr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
plogrpcr_prep <- prep(plogrpcr_rec)
bake(plogrpcr_prep, employee_train) %>%
slice(1:10)
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(plogrpcr_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross-validation (CV)
plogrpcr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
plogrpcr_cv_results <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
plogrpcr_best_tune <- plogrpcr_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
plogrpcr_final_wf <- plogrpcr_wf %>%
finalize_workflow(plogrpcr_best_tune) %>%
fit(data = employee_train)
# PREDICTIONS ---------------------------------------------------
# Predict without a classification cutoff--just the raw probabilities
plogrpcr_preds_no_c <- predict(plogrpcr_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=plogrpcr_preds_no_c, file="plogrpcr_preds_no_c_no_step_other.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(bike_train,
v = 5,
repeats = 1) # Split data for CV
# CROSS VALIDATION -------------------------------------------------
stack_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
library(stacks)
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(plogrpcr_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(ctree_rec) %>%
add_model(ctree_mod)
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = plogrpcr_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# CLASSIFICATION FOREST MODELING ------------------------------------
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 5)
# Run cross-validation
ctree_fit <- ctree_wf %>%
tune_grid(resamples = ctree_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(lme4)
library(stacks)
# Load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
stack1_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize
step_normalize(all_predictors()) %>%
# PCR
step_pca(all_predictors(), threshold = .94)
# Prep, Bake, and View Recipe
stack1_prep <- prep(stack1_rec)
bake(stack1_prep, employee_train) %>%
slice(1:10)
# CROSS VALIDATION -------------------------------------------------
stack1_folds <- vfold_cv(employee_train,
v = 5,
repeats = 1) # Split data for CV
stack1_untuned_model <- control_stack_grid() # Control grid for tuning over a grid
stack1_tuned_model <- control_stack_resamples() # Control grid for models we aren't tuning
# PENALIZED LOGISTIC REGRESSION MODELING ------------------------------------
# Create penalized logistic regression model
plogrpcr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogrpcr_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(plogrpcr_mod)
# Grid of values to tune over
plogrpcr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Tune model
plogrpcr_fit <- plogrpcr_wf %>%
tune_grid(resamples = stack1_folds,
grid = plogrpcr_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# CLASSIFICATION FOREST MODELING ------------------------------------
# Create classification forest model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 750) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(stack1_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 5)
# Run cross-validation
ctree_fit <- ctree_wf %>%
tune_grid(resamples = stack1_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc),
control = stack1_untuned_model)
# STACKED MODEL ----------------------------------------------
# Specify models to include
stack1_stack <- stacks() %>%
add_candidates(plogrpcr_fit) %>%
add_candidates(ctree_fit)
# Fit model w/ LASSO penalized regression meta-learner
stacked_model1 <- stack1_stack %>%
blend_predictions() %>%
fit_members()
# PREDICTIONS ---------------------------------------------------
# Predict without a classification cutoff--just the raw probabilities
stacked1_preds <- predict(stacked_model1,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=stacked1_preds, file="stacked1_preds.csv", delim = ",")
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
svms_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
library(embed)
# Create Recipe
svms_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
svms_prep <- prep(svms_rec)
bake(svms_prep, employee_train) %>%
slice(1:10)
# Create SVMS model
svms_mod <- svm_poly(degree = tune(), cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create SVMS model
svms_radial_mod <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create SVM workflow
svms_radial_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_radial_mod)
# Grid of values to tune over
svms_tg <- grid_regular(neighbors(),
levels = 10)
# Grid of values to tune over
svms_tg <- grid_regular(neighbors(),
levels = 5)
# Split data for cross-validation (CV)
svms_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Grid of values to tune over
svms_radial_tg <- grid_regular(neighbors(),
levels = 5)
# DATA CLEANING -------------------------------------------------
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
svms_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
svms_prep <- prep(svms_rec)
bake(svms_prep, employee_train) %>%
slice(1:10)
# MODELING ------------------------------------------------------
# Create SVMS model
svms_radial_mod <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create SVM workflow
svms_radial_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_radial_mod)
# Grid of values to tune over
svms_radial_tg <- grid_regular(neighbors(),
levels = 5)
# Split data for cross-validation (CV)
svms_radial_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
svms_radial_cv_results <- svms_radial_wf %>%
tune_grid(resamples = svms_radial_folds,
grid = svms_radial_tg,
metrics = metric_set(roc_auc))
install.packages('kernlab')
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kernlab)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
svms_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
svms_prep <- prep(svms_rec)
bake(svms_prep, employee_train) %>%
slice(1:10)
# MODELING ------------------------------------------------------
# Create SVMS model
svms_radial_mod <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create SVM workflow
svms_radial_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_radial_mod)
# Grid of values to tune over
svms_radial_tg <- grid_regular(neighbors(),
levels = 5)
# Split data for cross-validation (CV)
svms_radial_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
svms_radial_cv_results <- svms_radial_wf %>%
tune_grid(resamples = svms_radial_folds,
grid = svms_radial_tg,
metrics = metric_set(roc_auc))
# Load Libraries
library(vroom)
library(tidymodels)
library(tidyverse)
library(embed)
library(kernlab)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
svms_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
svms_prep <- prep(svms_rec)
bake(svms_prep, employee_train) %>%
slice(1:10)
# MODELING ------------------------------------------------------
# Create SVMS model
svms_radial_mod <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Create SVM workflow
svms_radial_wf <- workflow() %>%
add_recipe(svms_rec) %>%
add_model(svms_radial_mod)
# Grid of values to tune over
svms_radial_tg <- grid_regular(rbf_sigma(),
cost(),
levels = 5)
# Split data for cross-validation (CV)
svms_radial_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
svms_radial_cv_results <- svms_radial_wf %>%
tune_grid(resamples = svms_radial_folds,
grid = svms_radial_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
svms_radial_best_tune <- svms_radial_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
svms_radial_final_wf <- svms_radial_wf %>%
finalize_workflow(svms_radial_best_tune) %>%
fit(data = employee_train)
# PREDICTIONS ---------------------------------------------------
# Make predictions
svms_radial_preds <- predict(svms_radial_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=svms_radial_preds, file="svms_radial_preds.csv", delim = ",")
View(employee_train)
