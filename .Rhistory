# Combine categories that occur less than 1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .01) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
install.packages('embed')
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than 1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .01) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
library(embed)
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than 1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .01) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
plogr_prep <- prep(plogr_rec)
library(lme4)
# Prep, Bake, and View Recipe
plogr_prep <- prep(plogr_rec)
bake(plogr_prep, employee_train) %>%
slice(1:10)
x <- bake(plogr_prep, employee_train) #%>%
View(x)
# Create penalized logistic regression model
plogr_mod <- logistic_reg(mixture =, penalty =,) %>%
set_engine("glmnet")
# Create and fit logistic regression workflow
plogr_wf <- workflow() %>%
add_recipe(plogr_rec) %>%
add_model(plogr_mod)
# Grid of values to tune over
plogr_tg <- grid_regular(penalty(),
mixture(),
levels = 2)
# Grid of values to tune over
plogr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross-validation (CV)
folds <- vfold_cv(employee_train, v = 5, repeats = 1)
plogr_tg
# Split data for cross-validation (CV)
plogr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set(roc_auc))
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set(roc_auc, fmeas))
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set("roc_auc"))
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set(roc_auc))
# Create penalized logistic regression model
plogr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogr_wf <- workflow() %>%
add_recipe(plogr_rec) %>%
add_model(plogr_mod)
# Grid of values to tune over
plogr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross-validation (CV)
plogr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
plogr_best_tune <- plogr_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
plogr_final_wf <- plogr_wf %>%
finalize_workflow(plogr_best_tune) %>%
fit(data = employee_train)
# Predict without a classification cutoff--just the raw probabilities
plogr_preds_no_c <- predict(plogr_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=plogr_preds_no_c, file="plogr_preds_no_c.csv", delim = ",")
plogr_best_tune
rwolff17@becker.byu.edu
ssh rwolff17@becker.byu.edu
ssh rwolff17@becker.byu.edu
ssh rwolff17@becker.byu.edu
library(tidyverse)
# Load Libraries
library(tidymodels)
library(embed)
library(lme4)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Load Libraries
library(vroom)
library(DataExplorer)
library(patchwork)
library(tidyverse)
library(inspectdf)
library(ggmosaic)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
plogr_prep <- prep(plogr_rec)
bake(plogr_prep, employee_train) %>%
slice(1:10)
bake(plogr_prep, employee_train) %>%
slice(1:10)
# Create penalized logistic regression model
plogr_mod <- logistic_reg(mixture = tune(),
penalty = tune()) %>%
set_engine("glmnet")
# Create logistic regression workflow
plogr_wf <- workflow() %>%
add_recipe(plogr_rec) %>%
add_model(plogr_mod)
# Grid of values to tune over
plogr_tg <- grid_regular(penalty(),
mixture(),
levels = 5)
# Split data for cross-validation (CV)
plogr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
plogr_cv_results <- plogr_wf %>%
tune_grid(resamples = plogr_folds,
grid = plogr_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
plogr_best_tune <- plogr_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
plogr_final_wf <- plogr_wf %>%
finalize_workflow(plogr_best_tune) %>%
fit(data = employee_train)
# Predict without a classification cutoff--just the raw probabilities
plogr_preds_no_c <- predict(plogr_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=plogr_preds_no_c, file="plogr_preds_no_c_no_step_other.csv", delim = ",")
# Load Libraries
library(vroom)
library(vroom) # Loading data
library(DataExplorer) # EDA
library(patchwork) # EDA
library(inspectdf) # EDA
library(ggmosaic) # EDA
library(tidyverse) # General Use
library(tidymodels) # General Modeling
library(embed) # plogr modeling
library(lme4) # plogr modeling
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
# Remove because penalized logr can handle categories w few observations
# step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Create Recipe
plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
# Removed to see if classification trees can handle smaller data groups
# step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Create Recipe
ctree_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
# Removed to see if classification trees can handle smaller data groups
# step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
ctree_prep <- prep(ctree_prep)
# Prep, Bake, and View Recipe
ctree_prep <- prep(ctree_prep)
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
ctree_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
# Removed to see if classification trees can handle smaller data groups
# step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
ctree_prep <- prep(ctree_prep)
# Prep, Bake, and View Recipe
ctree_prep <- prep(ctree_rec)
bake(ctree_prep, employee_train) %>%
slice(1:10)
# Create penalized logistic regression model
ctree_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create classification forest workflow
ctree_wf <- workflow() %>%
add_recipe(ctree_rec) %>%
add_model(ctree_mod)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(),
min_n(),
levels = 5)
x <- bake(ctree_prep, employee_train) %>%
slice(1:10)
x
View(x)
# Grid of values to tune over
ctree_tg <- grid_regular(mtry(range = c(1, 9)),
min_n(),
levels = 5)
# Split data for cross-validation (CV)
ctree_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
ctree_cv_results <- ctree_wf %>%
tune_grid(resamples = ctree_folds,
grid = ctree_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
ctree_best_tune <- ctree_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
ctree_final_wf <- ctree_wf %>%
finalize_workflow(ctree_best_tune) %>%
fit(data = employee_train)
# Predict without a classification cutoff--just the raw probabilities
ctree_preds <- predict(ctree_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=ctree_preds, file="ctree_preds.csv", delim = ",")
# Load Libraries
library(doParallel) # Parallel Computing
library(vroom) # Loading data
library(DataExplorer) # EDA
library(patchwork) # EDA
library(inspectdf) # EDA
library(ggmosaic) # EDA
library(tidyverse) # General Use
library(tidymodels) # General Modeling
library(embed) # plogr modeling
library(lme4) # plogr modeling
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
nb_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, employee_train) %>%
slice(1:10)
install.packages("naivebayes")
# Load Libraries
# library(tidymodels)
# library(tidyverse)
library(naivebayes)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
install.packages("parsnip")
install.packages("parsnip")
library(parsnip)
library(doParallel) # Parallel Computing
library(vroom) # Loading data
library(DataExplorer) # EDA
library(patchwork) # EDA
library(inspectdf) # EDA
library(ggmosaic) # EDA
library(tidyverse) # General Use
library(tidymodels) # General Modeling
library(embed) # plogr modeling
library(lme4) # plogr modeling
library(naivebayes) # Naive Bayes modeling
library(parsnip) # Naive Bayes modeling
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
nb_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Prep, Bake, and View Recipe
nb_prep <- prep(nb_rec)
bake(nb_prep, employee_train) %>%
slice(1:10)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Create Naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
install.packages("discrim")
library(discrim) # Naive Bayes modeling
# Create Naive Bayes workflow
nb_wf <- workflow() %>%
add_recipe(nb_rec) %>%
add_model(nb_mod)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Grid of values to tune over
nb_tg <- grid_regular(Laplace(),
smoothness(),
levels = 10)
# Split data for cross-validation (CV)
nb_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
nb_cv_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tg,
metrics = metric_set(roc_auc))
# Find best tuning parameters
nb_best_tune <- nb_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
nb_final_wf <- nb_wf %>%
finalize_workflow(nb_best_tune) %>%
fit(data = employee_train)
# Make predictions
nb_preds <- predict(nb_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=nb_preds, file="nb_preds.csv", delim = ",")
install.packages("kknn")
# Load Libraries
library(doParallel) # Parallel Computing
library(vroom) # Loading data
library(DataExplorer) # EDA
library(patchwork) # EDA
library(inspectdf) # EDA
library(ggmosaic) # EDA
library(tidyverse) # General Use
library(tidymodels) # General Modeling
library(embed) # plogr modeling
library(lme4) # plogr modeling
library(naivebayes) # Naive Bayes modeling
library(discrim) # Naive Bayes modeling
library(kknn) # K nearest neighbors
# Re-load Data
employee_train <- vroom("train.csv")
employee_test <- vroom("test.csv")
# Change ACTION to factor before the recipe, as it isn't included in the test data set
employee_train$ACTION <- as.factor(employee_train$ACTION)
# Create Recipe
knn_rec <- recipe(ACTION ~ ., data = employee_train) %>%
# Vroom loads in data w numbers as numeric; turn all of these features into factors
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
# Combine categories that occur less than .1% of the time into an "other" category
step_other(all_nominal_predictors(), threshold = .001) %>%
# Target encoding for all nominal predictors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
# Normalize Numeric Predictors
step_normalize(all_numeric_predictors())
# Prep, Bake, and View Recipe
knn_prep <- prep(knn_rec)
bake(knn_prep, employee_train) %>%
slice(1:10)
# Create K Nearest Neighbors model
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_mode("classification") %>%
set_engine("kknn")
# Create KNN workflow
knn_wf <- workflow() %>%
add_recipe(knn_rec) %>%
add_model(knn_mod)
# Grid of values to tune over
knn_tg <- grid_regular(neighbors()),
# Grid of values to tune over
knn_tg <- grid_regular(neighbors(),
levels = 10)
# Split data for cross-validation (CV)
knn_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
# Run cross-validation
knn_cv_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tg,
metrics = metric_set(roc_auc))
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = employee_train)
# Find best tuning parameters
knn_best_tune <- knn_cv_results %>%
select_best("roc_auc")
# Finalize workflow and fit it
knn_final_wf <- knn_wf %>%
finalize_workflow(knn_best_tune) %>%
fit(data = employee_train)
# Make predictions
knn_preds <- predict(knn_final_wf,
new_data = employee_test,
type = "prob") %>%
bind_cols(employee_test$id, .) %>%
rename(Id = ...1) %>%
rename(Action = .pred_1) %>%
select(Id, Action)
# Create a CSV with the predictions
vroom_write(x=knn_preds, file="knn_preds.csv", delim = ",")
