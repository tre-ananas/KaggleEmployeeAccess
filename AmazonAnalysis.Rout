
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #################################################################
> #################################################################
> # Amazon Employee Access Challenge    ###########################
> # Ryan Wolff                          ###########################
> # 6 October 2023                      ###########################
> #################################################################
> #################################################################
> 
> #################################################################
> #################################################################
> # LOAD DATA                           ###########################
> #################################################################
> #################################################################
> 
> # Data Location and Description
> # https://www.kaggle.com/competitions/amazon-employee-access-challenge/data
> 
> # Load Libraries
> library(vroom)
> 
> # Load Data
> employee_train <- vroom("train.csv")
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> employee_test <- vroom("test.csv")
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> #################################################################
> #################################################################
> # EDA                                 ###########################
> #################################################################
> #################################################################
> 
> # # Load Libraries
> # library(DataExplorer)
> # library(patchwork)
> # library(tidyverse)
> # library(inspectdf)
> # library(ggmosaic)
> # 
> # # Create an EDA dataset and correct Vroom's mistake and make numeric data into factors
> # fact_employee_train_eda <- employee_train
> # cols <- c("ACTION",
> #           "RESOURCE", 
> #           "MGR_ID", 
> #           "ROLE_ROLLUP_1", 
> #           "ROLE_ROLLUP_2", 
> #           "ROLE_DEPTNAME", 
> #           "ROLE_TITLE",
> #           "ROLE_FAMILY_DESC",
> #           "ROLE_FAMILY",
> #           "ROLE_CODE")
> # fact_employee_train_eda[cols] <- lapply(fact_employee_train_eda[cols], 
> #                                    factor)
> # 
> # # Examine Factor Variables:
> #   # cnt = # unique variables
> #   # common = most common level
> #   # common_pcnt = percentage representing most common level
> #   # levels = list of the proportions of each level of the variable
> # factor_exploration_plot <- fact_employee_train_eda %>%
> #   inspect_cat() %>%
> #   show_plot()
> # factor_exploration_plot
> # 
> # fact_employee_train_eda %>%
> #   inspect_cat()
> # 
> # # Create an EDA dataset making every feature into a factor except ACTION, which remains numeric
> # num_employee_train_eda <- employee_train
> # cols <- c("RESOURCE", 
> #           "MGR_ID", 
> #           "ROLE_ROLLUP_1", 
> #           "ROLE_ROLLUP_2", 
> #           "ROLE_DEPTNAME", 
> #           "ROLE_TITLE",
> #           "ROLE_FAMILY_DESC",
> #           "ROLE_FAMILY",
> #           "ROLE_CODE")
> # num_employee_train_eda[cols] <- lapply(num_employee_train_eda[cols], 
> #                                    factor)
> # 
> # # Identify the top 30 most popular RESOURCEs
> # resource_data <- num_employee_train_eda %>%
> #   group_by(RESOURCE) %>%
> #   summarize(mean = mean(ACTION),
> #             n = n()) %>%
> #   slice_max(order_by = n,
> #             n = 30)
> # resource_data <- resource_data["RESOURCE"]
> # 
> # # Subset the EDA dataset to the 30 most popular RESOURCEs
> # num_employee_train_eda <- num_employee_train_eda %>%
> #   filter(RESOURCE %in% resource_data$RESOURCE)
> # 
> # # Bar chart of the top 30 most popular RESOURCEs' ACTION results
> # action_resources_barcharts <- ggplot(data = num_employee_train_eda,
> #        aes(x = ACTION)) +
> #   geom_bar() +
> #   ggtitle("ACTION Results for 30 Most Common Products") +
> #   xlab("ACTION: 0 and 1, Respectively") +
> #   ylab("Count of Each ACTION Result") +
> #   theme(plot.title = element_text(hjust = .5)) +
> #   facet_wrap( ~ RESOURCE)
> # action_resources_barcharts
> # 
> # # Create a 2-Way Plot of Prominent Plots
> # twoway_patch <- (factor_exploration_plot) / (action_resources_barcharts)
> # twoway_patch
>   
> # Findings:
>   # inspect_cat():
>     # ACTION is extremely homogenous--94.2% are in value 1, aka the resource was approved
>     # ROLE_ROLLUP_1 is extremely homogenous--65.3% are in value 117961
>   # show_plot() is a visualization of inspect_cat()
>   # geom_bar():
>     # 20897 is the only RESOURCE that is not overwhelmingly approved
> 
> #################################################################
> #################################################################
> # LOGISTIC REGRESSION                 ###########################
> #################################################################
> #################################################################
> 
> # DATA CLEANING -------------------------------------------------
> 
> # Load Libraries
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ recipes      1.0.8
✔ dials        1.2.0     ✔ rsample      1.2.0
✔ dplyr        1.1.3     ✔ tibble       3.2.1
✔ ggplot2      3.4.3     ✔ tidyr        1.3.0
✔ infer        1.0.5     ✔ tune         1.1.2
✔ modeldata    1.2.0     ✔ workflows    1.1.3
✔ parsnip      1.1.1     ✔ workflowsets 1.0.1
✔ purrr        1.0.2     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::discard()  masks scales::discard()
✖ dplyr::filter()   masks stats::filter()
✖ dplyr::lag()      masks stats::lag()
✖ yardstick::spec() masks vroom::spec()
✖ recipes::step()   masks stats::step()
• Use suppressPackageStartupMessages() to eliminate package startup messages
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ forcats   1.0.0     ✔ readr     2.1.4
✔ lubridate 1.9.3     ✔ stringr   1.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ readr::col_character()   masks vroom::col_character()
✖ readr::col_date()        masks vroom::col_date()
✖ readr::col_datetime()    masks vroom::col_datetime()
✖ readr::col_double()      masks vroom::col_double()
✖ readr::col_factor()      masks scales::col_factor(), vroom::col_factor()
✖ readr::col_guess()       masks vroom::col_guess()
✖ readr::col_integer()     masks vroom::col_integer()
✖ readr::col_logical()     masks vroom::col_logical()
✖ readr::col_number()      masks vroom::col_number()
✖ readr::col_skip()        masks vroom::col_skip()
✖ readr::col_time()        masks vroom::col_time()
✖ readr::cols()            masks vroom::cols()
✖ readr::date_names_lang() masks vroom::date_names_lang()
✖ readr::default_locale()  masks vroom::default_locale()
✖ purrr::discard()         masks scales::discard()
✖ dplyr::filter()          masks stats::filter()
✖ stringr::fixed()         masks recipes::fixed()
✖ readr::fwf_cols()        masks vroom::fwf_cols()
✖ readr::fwf_empty()       masks vroom::fwf_empty()
✖ readr::fwf_positions()   masks vroom::fwf_positions()
✖ readr::fwf_widths()      masks vroom::fwf_widths()
✖ dplyr::lag()             masks stats::lag()
✖ readr::locale()          masks vroom::locale()
✖ readr::output_column()   masks vroom::output_column()
✖ readr::problems()        masks vroom::problems()
✖ readr::spec()            masks yardstick::spec(), vroom::spec()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> 
> # Re-load Data
> employee_train <- vroom("train.csv")
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> employee_test <- vroom("test.csv")
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # Change ACTION to factor before the recipe, as it isn't included in the test data set
> employee_train$ACTION <- as.factor(employee_train$ACTION)
> 
> # Create Recipe
> logr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
+   # Vroom loads in data w numbers as numeric; turn all of these features into factors
+   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
+   # Combine categories that occur less than 1% of the time into an "other" category
+   step_other(all_nominal_predictors(), threshold = .01) %>%
+   # Dummy variable encoding for all nominal predictors
+   step_dummy(all_nominal_predictors())
> 
> 
> # Prep, Bake, and View Recipe
> logr_prep <- prep(logr_rec)
> bake(logr_prep, employee_train) %>%
+   slice(1:10)
# A tibble: 10 × 112
   ACTION RESOURCE_X4675 RESOURCE_X25993 RESOURCE_X75078 RESOURCE_X79092
   <fct>           <dbl>           <dbl>           <dbl>           <dbl>
 1 1                   0               0               0               0
 2 1                   0               0               0               0
 3 1                   0               0               0               0
 4 1                   0               0               0               0
 5 1                   0               0               0               0
 6 0                   0               0               0               0
 7 1                   0               1               0               0
 8 1                   0               0               0               0
 9 1                   0               0               0               0
10 1                   0               0               0               0
# ℹ 107 more variables: RESOURCE_other <dbl>, MGR_ID_other <dbl>,
#   ROLE_ROLLUP_1_X117902 <dbl>, ROLE_ROLLUP_1_X117961 <dbl>,
#   ROLE_ROLLUP_1_X118212 <dbl>, ROLE_ROLLUP_1_X118290 <dbl>,
#   ROLE_ROLLUP_1_X118315 <dbl>, ROLE_ROLLUP_1_X118887 <dbl>,
#   ROLE_ROLLUP_1_X119062 <dbl>, ROLE_ROLLUP_1_other <dbl>,
#   ROLE_ROLLUP_2_X117962 <dbl>, ROLE_ROLLUP_2_X117969 <dbl>,
#   ROLE_ROLLUP_2_X118026 <dbl>, ROLE_ROLLUP_2_X118052 <dbl>, …
> 
> # MODELING ------------------------------------------------------
> 
> # Create logistic regression model
> logr_mod <- logistic_reg() %>%
+   set_engine("glm")
> 
> # Create and fit logistic regression workflow
> logr_wf <- workflow() %>%
+   add_recipe(logr_rec) %>%
+   add_model(logr_mod) %>%
+   fit(data = employee_train)
> 
> # Predict with classification cutoff = .70
> logr_preds <- predict(logr_wf,
+                      new_data = employee_test,
+                      type = "prob") %>%
+   mutate(ifelse(.pred_1 > .83, 1, 0)) %>%
+   bind_cols(employee_test$id, .) %>%
+   rename(Id = ...1) %>%
+   rename(Action = names(.)[4]) %>%
+   select(Id, Action)
New names:
• `` -> `...1`
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = if (type ==  :
  prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
> 
> # Create a CSV with the predictions
> # vroom_write(x=logr_preds, file="logr_preds.csv", delim = ",")
> 
> # Predict without a classification cutoff--just the raw probabilities
> logr_preds_no_c <- predict(logr_wf,
+                      new_data = employee_test,
+                      type = "prob") %>%
+   bind_cols(employee_test$id, .) %>%
+   rename(Id = ...1) %>%
+   rename(Action = .pred_1) %>%
+   select(Id, Action)
New names:
• `` -> `...1`
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = if (type ==  :
  prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
> 
> # Create a CSV with the predictions
> vroom_write(x=logr_preds_no_c, file="logr_preds_no_c.csv", delim = ",")
> 
> #################################################################
> #################################################################
> # PENALIZED LOGISTIC REGRESSION       ###########################
> #################################################################
> #################################################################
> 
> # DATA CLEANING -------------------------------------------------
> 
> # Load Libraries
> library(tidymodels)
> library(tidyverse)
> library(embed)
> library(lme4)
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

> 
> # Re-load Data
> employee_train <- vroom("train.csv")
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> employee_test <- vroom("test.csv")
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # Change ACTION to factor before the recipe, as it isn't included in the test data set
> employee_train$ACTION <- as.factor(employee_train$ACTION)
> 
> # Create Recipe
> plogr_rec <- recipe(ACTION ~ ., data = employee_train) %>%
+   # Vroom loads in data w numbers as numeric; turn all of these features into factors
+   step_mutate_at(all_numeric_predictors(), fn = factor) %>%
+   # Combine categories that occur less than .1% of the time into an "other" category
+   # Remove because penalized logr can handle categories w few observations
+   # step_other(all_nominal_predictors(), threshold = .001) %>%
+   # Target encoding for all nominal predictors
+   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
> 
> 
> # Prep, Bake, and View Recipe
> plogr_prep <- prep(plogr_rec)
> bake(plogr_prep, employee_train) %>%
+   slice(1:10)
# A tibble: 10 × 10
   RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
      <dbl>  <dbl>         <dbl>         <dbl>         <dbl>      <dbl>
 1    -3.65 -6.28          -2.93         -3.08         -3.16      -3.40
 2    -3.40 -5.62          -2.93         -3.44         -2.17      -3.29
 3    -3.60 -5.40          -2.48         -2.47         -2.50      -2.09
 4    -3.54 -6.34          -2.93         -3.44         -4.27      -2.45
 5    -3.85 -5.59          -2.63         -2.05         -1.29      -2.00
 6    -2.28 -0.949         -2.71         -2.68         -2.12      -2.52
 7    -3.03 -5.84          -2.93         -3.44         -3.98      -2.58
 8    -4.10 -5.51          -2.93         -2.56         -3.26      -2.69
 9    -3.70 -6.00          -2.93         -3.29         -3.07      -2.37
10    -4.00 -5.32          -1.52         -1.51         -2.71      -2.09
# ℹ 4 more variables: ROLE_FAMILY_DESC <dbl>, ROLE_FAMILY <dbl>,
#   ROLE_CODE <dbl>, ACTION <fct>
> 
> # MODELING ------------------------------------------------------
> 
> # Create penalized logistic regression model
> plogr_mod <- logistic_reg(mixture = tune(),
+                           penalty = tune()) %>%
+   set_engine("glmnet")
> 
> # Create logistic regression workflow
> plogr_wf <- workflow() %>%
+   add_recipe(plogr_rec) %>%
+   add_model(plogr_mod)
> 
> # Grid of values to tune over
> plogr_tg <- grid_regular(penalty(),
+                          mixture(),
+                          levels = 5)
> 
> # Split data for cross-validation (CV)
> plogr_folds <- vfold_cv(employee_train, v = 5, repeats = 1)
> 
> # Run cross-validation
> plogr_cv_results <- plogr_wf %>%
+   tune_grid(resamples = plogr_folds,
+             grid = plogr_tg,
+             metrics = metric_set(roc_auc))
> 
> # Find best tuning parameters
> plogr_best_tune <- plogr_cv_results %>%
+   select_best("roc_auc")
> 
> # Finalize workflow and fit it
> plogr_final_wf <- plogr_wf %>%
+   finalize_workflow(plogr_best_tune) %>%
+   fit(data = employee_train)
> 
> # Predict without a classification cutoff--just the raw probabilities
> plogr_preds_no_c <- predict(plogr_final_wf,
+                      new_data = employee_test,
+                      type = "prob") %>%
+   bind_cols(employee_test$id, .) %>%
+   rename(Id = ...1) %>%
+   rename(Action = .pred_1) %>%
+   select(Id, Action)
New names:
• `` -> `...1`
> 
> # Create a CSV with the predictions
> vroom_write(x=plogr_preds_no_c, file="plogr_preds_no_c_no_step_other.csv", delim = ",")
> 
> proc.time()
   user  system elapsed 
425.108   4.501 236.918 
